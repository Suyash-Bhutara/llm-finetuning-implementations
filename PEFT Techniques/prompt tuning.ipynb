{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Prompt Tuning"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, we will look into how to perform prompt tuning for a text classification task."]},{"cell_type":"markdown","metadata":{},"source":["Load the required libraries and the config parameters"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:02.263506Z","iopub.status.busy":"2024-06-16T13:37:02.262738Z","iopub.status.idle":"2024-06-16T13:37:02.270602Z","shell.execute_reply":"2024-06-16T13:37:02.269417Z","shell.execute_reply.started":"2024-06-16T13:37:02.263470Z"},"trusted":true},"outputs":[],"source":["import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n","from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n","import torch\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","import wandb\n","\n","wandb.init(project=\"prompt_learning_methods\", name=\"prompt_tuning\")\n","seed = 42\n","device = \"cuda\"\n","model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n","tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n","dataset_name = \"twitter_complaints\"\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","max_length = 64\n","lr = 1e-4\n","num_epochs = 10\n","batch_size = 8\n","set_seed(seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Preparation"]},{"cell_type":"markdown","metadata":{},"source":["### Load the dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:07.301614Z","iopub.status.busy":"2024-06-16T13:37:07.300948Z","iopub.status.idle":"2024-06-16T13:37:09.471227Z","shell.execute_reply":"2024-06-16T13:37:09.470209Z","shell.execute_reply.started":"2024-06-16T13:37:07.301582Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Unlabeled', 'complaint', 'no complaint']\n","DatasetDict({\n","    train: Dataset({\n","        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n","        num_rows: 50\n","    })\n","    test: Dataset({\n","        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n","        num_rows: 3399\n","    })\n","})\n"]},{"data":{"text/plain":["{'Tweet text': '@HMRCcustomers No this is my first job',\n"," 'ID': 0,\n"," 'Label': 2,\n"," 'text_label': 'no complaint'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"ought/raft\", dataset_name)\n","\n","classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","print(classes)\n","dataset = dataset.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","print(dataset)\n","dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:14.253600Z","iopub.status.busy":"2024-06-16T13:37:14.252730Z","iopub.status.idle":"2024-06-16T13:37:14.260452Z","shell.execute_reply":"2024-06-16T13:37:14.259434Z","shell.execute_reply.started":"2024-06-16T13:37:14.253565Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Counter({2: 33, 1: 17})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from collections import Counter\n","Counter(dataset[\"train\"][\"Label\"])"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocess the dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:17.017766Z","iopub.status.busy":"2024-06-16T13:37:17.017104Z","iopub.status.idle":"2024-06-16T13:37:18.248555Z","shell.execute_reply":"2024-06-16T13:37:18.247574Z","shell.execute_reply.started":"2024-06-16T13:37:17.017733Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["target_max_length=4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e223773cf3174f0591a5eadc7e9b737d","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"139971990e894bdea23d8df8668ec199","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     1,   320, 12523,  2245,   714,   802,   642, 28711,   311,\n","           8503,  1080,  6304,   272,  9827,   354,   528,    13,  4565,   714,\n","          28705,   708, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n","            714,   802,  2707,  6024, 28754,   525, 28709, 28743,  4585, 15359,\n","           8196,   354,  1558,  4089, 28725,   829,   347,   586,  7865,   562,\n","           3062,  2368, 28742, 28707,  1709,   298,   347,  2739,    13,  4565,\n","            714, 28705, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     1,   320, 12523,  2245,   714,   802,  9979,  1242,\n","           8062,   802,   675,  2867,   399,  2665,   528,    13,  4565,   714,\n","          28705,   708, 22105,     2],\n","         [    2,     2,     2,     1,   320, 12523,  2245,   714,  1450,  3231,\n","            288, 28808, 23292, 28705, 28750, 10712,  6434,   354, 19338,   297,\n","            422,  6263, 28708,   294,  4120, 28725,   422, 28759, 28798,   422,\n","           6487,   374,   380,   422, 11966,   274, 28722,   734,   883,  6043,\n","          14716, 28808, 28878,  4449,  1508, 28707, 28723,  1115, 28748,  6042,\n","          28781,  2228, 28758, 28814, 28729, 28755, 28729,    13,  4565,   714,\n","          28705,   708, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     1,   320, 12523,  2245,   714,   802,   811,\n","          28709, 28730, 11538,   802,  4888,  1536,   574, 12688,   349,  9783,\n","          18000,   349,  2115,   304,  1149,  1012,  4308,   290,  1126, 28723,\n","           2483,   378,  2553,   378, 28742, 28713,  1318,  4876,    13,  4565,\n","            714, 28705, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n","            714,   802, 28765,   279,  2581,  9452,   739,   460,   368,  8817,\n","            288,   633,  9917, 10190,   354,  6735,  2668,    13,  4565,   714,\n","          28705,   708, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     1,   320, 12523,  2245,   714,  3194,\n","          10198,   264,  8428,  4449,  1508, 28707, 28723,  1115, 28748, 28754,\n","           1981, 28765, 28727, 28743, 28768,  7209, 28718,    13,  4565,   714,\n","          28705,   708, 22105,     2],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     1,   320,\n","          12523,  2245,   714, 12376,  2815,  3500, 19521, 28725,   579, 17949,\n","          28723, 15108,   422, 21404,  2199,  3167, 14265,    13,  4565,   714,\n","          28705,   708, 22105,     2]]),\n"," 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n"," 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2],\n","         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","           -100,   708, 22105,     2]])}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","print(f\"{target_max_length=}\")\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"train\"]\n","\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","next(iter(train_dataloader))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:23.911366Z","iopub.status.busy":"2024-06-16T13:37:23.910315Z","iopub.status.idle":"2024-06-16T13:37:24.896252Z","shell.execute_reply":"2024-06-16T13:37:24.895155Z","shell.execute_reply.started":"2024-06-16T13:37:23.911331Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aeb875cca76d42b09e41e8e0abbe1605","version_major":2,"version_minor":0},"text/plain":["Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     1,   320, 12523,  2245,   714,   802, 28760,   324, 14233,\n","           2328,  8868,   354, 10313,   586,  7416,  2169,   395,   272,  4908,\n","           8147,  1309,   356,   378, 28808, 11936, 28723, 22747, 28723,   675,\n","          28748, 28710, 28737, 28734, 11788, 28744, 28762, 28779, 28779, 28750,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n","            714,   334,   855,   288,   582,   356,   422,  3836,  3957,  1829,\n","          19653, 28808,   415,   905,   590,  3088,   354,  1167,  4370,   568,\n","           1371,   272,  8710,   472, 28705, 29137, 29137, 29274, 30155, 29096,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     1,   320, 12523,  2245,   714, 15359,   802, 21270, 28713,\n","          17302, 28725,   349,   378,  1055, 20775, 28713,  4920,   298,   865,\n","           2405,   264, 20106,   513,   272,  2515,   349,  9683, 28724, 28804,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              1,   320, 12523,  2245,   714,   802, 28798,  9399, 24855,  5156,\n","            315, 28809, 28719, 13903,  1101,   335,   272,  1537, 28735,   349,\n","            272,   981,   291,   529, 17381,  3372,   511,   368,  3091,   369,\n","           8623, 28733,  8820,  6835,   460, 17381, 28878,  4449,  1508, 28707,\n","          28723,  1115, 28748,   299, 17580, 28728, 28754,  2252, 28740, 28716,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     1,   320, 12523,\n","           2245,   714,   802, 13052,   795,   602,  7261,  6504,  8196,  1101,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n","              2,     2,     1,   320, 12523,  2245,   714,   802, 28719,   641,\n","            301,   463,  9255,   490, 28706,   613, 28809, 28719,  2590, 15671,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n","            320, 12523,  2245,   714,   802, 28757, 14595, 28735,  3851,   298,\n","           3848,   456,  3154, 28723, 28301,   272, 24942,   773, 28705, 28781,\n","          28734, 28823,   805,   954, 24104, 28723,  7336,  2240,   295,  1449,\n","            460,   459,  4525,   345,   763, 24104, 28739,  4449,  1508, 28707,\n","          28723,  1115, 28748, 28728,   510, 16457, 28740, 28765,  1737, 28787,\n","             13,  4565,   714, 28705],\n","         [    2,     2,     2,     2,     2,     2,     2,     2,     1,   320,\n","          12523,  2245,   714,   802, 28719,  3865,   322,   802, 28719,  3810,\n","            444, 28721,  9050,   851,   349,   272,  1080,  7714,  8710,  1970,\n","            315, 28742,   333,  2270,  3364,   302, 28723,  8580,  3707,   349,\n","           1038,  2525,  7656,  4564,  2117, 28878,  4449,  1508, 28707, 28723,\n","           1115, 28748, 28719, 28783, 28715,  2547, 28734,   278, 28779, 28726,\n","             13,  4565,   714, 28705]]),\n"," 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["def test_preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n","    model_inputs = tokenizer(inputs)\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","    return model_inputs\n","\n","\n","test_dataset = dataset[\"test\"].map(\n","    test_preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","next(iter(test_dataloader))"]},{"cell_type":"markdown","metadata":{},"source":["## Create the PEFT model, Optimizer and LR Scheduler"]},{"cell_type":"markdown","metadata":{},"source":["## Prompt Tuning config "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:29.964139Z","iopub.status.busy":"2024-06-16T13:37:29.963377Z","iopub.status.idle":"2024-06-16T13:37:29.969459Z","shell.execute_reply":"2024-06-16T13:37:29.968456Z","shell.execute_reply.started":"2024-06-16T13:37:29.964106Z"},"trusted":true},"outputs":[],"source":["prompt_tuning_init_text=\"Classify if the tweet is a complaint or no complaint.\\n\"\n","peft_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    prompt_tuning_init=PromptTuningInit.TEXT,\n","    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n","    prompt_tuning_init_text=prompt_tuning_init_text,\n","    tokenizer_name_or_path=model_name_or_path,\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:37:38.942483Z","iopub.status.busy":"2024-06-16T13:37:38.942095Z","iopub.status.idle":"2024-06-16T13:38:36.953908Z","shell.execute_reply":"2024-06-16T13:38:36.952759Z","shell.execute_reply.started":"2024-06-16T13:37:38.942451Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99a9aa54947547c79db629ce9eb1189e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["trainable params: 61,440 || all params: 7,241,793,536 || trainable%: 0.0008\n"]}],"source":["# creating model\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16)\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\":False})"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:38:36.957126Z","iopub.status.busy":"2024-06-16T13:38:36.956524Z","iopub.status.idle":"2024-06-16T13:38:41.760753Z","shell.execute_reply":"2024-06-16T13:38:41.759818Z","shell.execute_reply.started":"2024-06-16T13:38:36.957096Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): MistralForCausalLM(\n","    (model): MistralModel(\n","      (embed_tokens): Embedding(32000, 4096)\n","      (layers): ModuleList(\n","        (0-31): 32 x MistralDecoderLayer(\n","          (self_attn): MistralSdpaAttention(\n","            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","            (rotary_emb): MistralRotaryEmbedding()\n","          )\n","          (mlp): MistralMLP(\n","            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","            (act_fn): SiLU()\n","          )\n","          (input_layernorm): MistralRMSNorm()\n","          (post_attention_layernorm): MistralRMSNorm()\n","        )\n","      )\n","      (norm): MistralRMSNorm()\n","    )\n","    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","  )\n","  (prompt_encoder): ModuleDict(\n","    (default): PromptEmbedding(\n","      (embedding): Embedding(15, 4096)\n","    )\n","  )\n","  (word_embeddings): Embedding(32000, 4096)\n",")"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model = model.to(device)\n","model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:38:46.439120Z","iopub.status.busy":"2024-06-16T13:38:46.438173Z","iopub.status.idle":"2024-06-16T13:38:46.447193Z","shell.execute_reply":"2024-06-16T13:38:46.446087Z","shell.execute_reply.started":"2024-06-16T13:38:46.439086Z"},"trusted":true},"outputs":[],"source":["# optimizer and lr scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Qualitative evaluation on test samples before finetuning"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:38:54.164436Z","iopub.status.busy":"2024-06-16T13:38:54.163557Z","iopub.status.idle":"2024-06-16T13:38:55.329829Z","shell.execute_reply":"2024-06-16T13:38:55.328734Z","shell.execute_reply.started":"2024-06-16T13:38:54.164404Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1533: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n","  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"]},{"name":"stdout","output_type":"stream","text":["Tweet text : @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n","Label : 1\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"source":["model.eval()\n","i = 33\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=20, eos_token_id=tokenizer.eos_token_id\n","    )\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Evaluation loop"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:39:03.182766Z","iopub.status.busy":"2024-06-16T13:39:03.181871Z","iopub.status.idle":"2024-06-16T13:44:03.266928Z","shell.execute_reply":"2024-06-16T13:44:03.265574Z","shell.execute_reply.started":"2024-06-16T13:39:03.182734Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","100%|██████████| 7/7 [00:21<00:00,  3.08s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=0: train_ppl=tensor(37.3852, device='cuda:0') train_epoch_loss=tensor(3.6213, device='cuda:0') eval_ppl=tensor(9.5689, device='cuda:0') eval_epoch_loss=tensor(2.2585, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=1: train_ppl=tensor(4.6405, device='cuda:0') train_epoch_loss=tensor(1.5348, device='cuda:0') eval_ppl=tensor(2.4861, device='cuda:0') eval_epoch_loss=tensor(0.9107, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=2: train_ppl=tensor(1.9083, device='cuda:0') train_epoch_loss=tensor(0.6462, device='cuda:0') eval_ppl=tensor(1.6095, device='cuda:0') eval_epoch_loss=tensor(0.4759, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=3: train_ppl=tensor(1.4684, device='cuda:0') train_epoch_loss=tensor(0.3842, device='cuda:0') eval_ppl=tensor(1.3163, device='cuda:0') eval_epoch_loss=tensor(0.2749, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=4: train_ppl=tensor(1.2632, device='cuda:0') train_epoch_loss=tensor(0.2337, device='cuda:0') eval_ppl=tensor(1.2962, device='cuda:0') eval_epoch_loss=tensor(0.2595, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=5: train_ppl=tensor(1.2270, device='cuda:0') train_epoch_loss=tensor(0.2046, device='cuda:0') eval_ppl=tensor(1.1830, device='cuda:0') eval_epoch_loss=tensor(0.1681, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=6: train_ppl=tensor(1.1738, device='cuda:0') train_epoch_loss=tensor(0.1603, device='cuda:0') eval_ppl=tensor(1.1237, device='cuda:0') eval_epoch_loss=tensor(0.1166, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=7: train_ppl=tensor(1.1316, device='cuda:0') train_epoch_loss=tensor(0.1236, device='cuda:0') eval_ppl=tensor(1.0981, device='cuda:0') eval_epoch_loss=tensor(0.0936, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch=8: train_ppl=tensor(1.0905, device='cuda:0') train_epoch_loss=tensor(0.0866, device='cuda:0') eval_ppl=tensor(1.0768, device='cuda:0') eval_epoch_loss=tensor(0.0740, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:21<00:00,  3.06s/it]\n","100%|██████████| 7/7 [00:08<00:00,  1.22s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch=9: train_ppl=tensor(1.0869, device='cuda:0') train_epoch_loss=tensor(0.0833, device='cuda:0') eval_ppl=tensor(1.0727, device='cuda:0') eval_epoch_loss=tensor(0.0702, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# training and evaluation\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.autocast(dtype=torch.float16, device_type=\"cuda\"):\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n","    wandb.log({\"train\": {\"perplexity\": train_ppl, \"loss\": train_epoch_loss, \"epoch\": epoch}, \n","               \"val\": {\"perplexity\": eval_ppl, \"loss\": eval_epoch_loss, \"epoch\": epoch}})\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Qualitative evaluation on test samples after finetuning"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:44:03.269343Z","iopub.status.busy":"2024-06-16T13:44:03.269030Z","iopub.status.idle":"2024-06-16T13:44:03.557419Z","shell.execute_reply":"2024-06-16T13:44:03.556278Z","shell.execute_reply.started":"2024-06-16T13:44:03.269315Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s> Tweet text : @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n","Label :  complaint</s>\n"]}],"source":["model.eval()\n","i = 33\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=5, eos_token_id=tokenizer.eos_token_id\n","    )\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the model and optionally pushing it to Hub"]},{"cell_type":"markdown","metadata":{},"source":["You can push model to hub or save model locally. \n","\n","- Option1: Pushing the model to Hugging Face Hub\n","```python\n","model.push_to_hub(\n","    f\"mistral_prompt_tuning\",\n","    token = \"hf_...\"\n",")\n","```\n","token (`bool` or `str`, *optional*):\n","    `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\n","    when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n","    is not specified.\n","    Or you can get your token from https://huggingface.co/settings/token\n","```\n","- Or save model locally\n","```python\n","peft_model_id = f\"mistral_prompt_tuning\"\n","model.save_pretrained(peft_model_id)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# saving model\n","peft_model_id = \"mistral_prompt_tuning\"\n","model.push_to_hub(peft_model_id, private=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Check the size of the checkpoint"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:44:03.559010Z","iopub.status.busy":"2024-06-16T13:44:03.558688Z","iopub.status.idle":"2024-06-16T13:44:04.706352Z","shell.execute_reply":"2024-06-16T13:44:04.705059Z","shell.execute_reply.started":"2024-06-16T13:44:03.558984Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Sun Jun 16 13:44:04 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   62C    P0              44W / 250W |  15656MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Load the PEFT checkpoint and do the qualitative analysis on test samples"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:29:19.799602Z","iopub.status.busy":"2024-06-16T13:29:19.799137Z","iopub.status.idle":"2024-06-16T13:30:24.512733Z","shell.execute_reply":"2024-06-16T13:30:24.511858Z","shell.execute_reply.started":"2024-06-16T13:29:19.799563Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad2fde5f72bf44fbad388772ed25dc1f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from datasets import load_dataset\n","import torch\n","\n","dataset = load_dataset(\"ought/raft\", \"twitter_complaints\")\n","peft_model_id = \"bazinga1998/mistral_prompt_tuning\"\n","device = \"cuda\"\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float16)\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:30:24.515698Z","iopub.status.busy":"2024-06-16T13:30:24.514544Z","iopub.status.idle":"2024-06-16T13:30:29.906603Z","shell.execute_reply":"2024-06-16T13:30:29.905479Z","shell.execute_reply.started":"2024-06-16T13:30:24.515655Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1533: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n","  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"]},{"name":"stdout","output_type":"stream","text":["Tweet text : @virginmedia Instead of spending money on advertising, why not fix the slow speeds in the RG2 area. CLOWNS\n","Label :  complaint\n"]}],"source":["model.to(device)\n","model.eval()\n","i = 36\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ', return_tensors=\"pt\")\n","# print(dataset[\"test\"][i][\"Tweet text\"])\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=tokenizer.eos_token_id\n","    )\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T13:32:31.758421Z","iopub.status.busy":"2024-06-16T13:32:31.758011Z","iopub.status.idle":"2024-06-16T13:32:32.884233Z","shell.execute_reply":"2024-06-16T13:32:32.883229Z","shell.execute_reply.started":"2024-06-16T13:32:31.758378Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Sun Jun 16 13:32:32 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0              31W / 250W |  14912MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2254034,"sourceId":3774014,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
