{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28506e0e-b778-4803-84b1-4c859d2741fd",
   "metadata": {},
   "source": [
    "# Instruction Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96",
   "metadata": {},
   "source": [
    "In this notebook, we will look into how to perform instruction finetuning. We will be doing full finetuning, i.e., retraining all the paramters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098e055-a939-4da7-879e-85849982cdcb",
   "metadata": {},
   "source": [
    "Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"tinyllama_instruct_finetuning\"\n",
    "\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991163c6-29f3-496e-b71c-f4329ec25df1",
   "metadata": {},
   "source": [
    "## Data preprocessing: Creating Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b27033-ba17-4c85-986d-9cbef9262497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T\"\n",
    "dataset_name = \"HuggingFaceH4/no_robots\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
    "tokenizer.chat_template = template\n",
    "\n",
    "def preprocess(samples):\n",
    "    batch = []\n",
    "    for conversation in samples[\"messages\"]:\n",
    "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
    "    return {\"content\": batch}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa818a8d-5f2d-4cb8-9283-78f897f6618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a",
   "metadata": {},
   "source": [
    "## Loading the pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "        bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "        eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "tokenizer.chat_template = template\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9157a4c-83f0-424c-8942-d528f0ab3d95",
   "metadata": {},
   "source": [
    "## Storing the base model predictions on a subset of 25 samples from eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e3f6c-3b9c-4d20-8289-fd91a00e63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side=\"left\"\n",
    "def get_prediction_batched(samples, column_name):\n",
    "    batch = []\n",
    "    for conversation in samples[\"messages\"]:\n",
    "        chatml_gen_prompt = tokenizer.apply_chat_template(conversation[:-1], tokenize=False, add_generation_prompt=True)\n",
    "        batch.append(chatml_gen_prompt)\n",
    "    #text = tokenizer.apply_chat_template(conversation_history, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)#, add_special_tokens=False)\n",
    "    inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=100, \n",
    "                             do_sample=True, \n",
    "                             top_p=0.95, \n",
    "                             temperature=0.2, \n",
    "                             repetition_penalty=1.1, \n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                            )\n",
    "    outputs = tokenizer.batch_decode(outputs)\n",
    "    outputs = [output.split(\"<|im_start|>assistant\")[-1].split(\"<|im_end|>\")[0].strip() for output in outputs]\n",
    "    return {column_name: outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e683373-e6aa-4847-835f-5fef56d170f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "test_dataset = load_dataset(dataset_name)[\"test\"].shuffle().select(range(1))\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(get_prediction_batched, column_name=\"base_assistant_message\"),\n",
    "    batched=True,\n",
    "    batch_size=1)\n",
    "\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8987259-21a6-415d-abcf-7517588b60da",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c590d-16cb-4edc-9aab-30b8137a2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tinyllama_instruct\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "logging_steps = 25\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "max_steps = 250\n",
    "num_train_epochs=1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    fp16=True,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    # hub_private_repo=True,\n",
    "    # push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # no_cuda=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"content\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480d10e-48ad-46e6-bbfc-7e74b148f9b1",
   "metadata": {},
   "source": [
    "## Loading the trained model and getting the predictions of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b09ee-47f4-4f2b-a7e8-d0b3884f7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(\n",
    "    partial(get_prediction_batched, column_name=\"instruct_assistant_message\"),\n",
    "    batched=True,\n",
    "    batch_size=1)\n",
    "\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53127053-dd54-421e-88a3-389bade47e59",
   "metadata": {},
   "source": [
    "## Comparing the outputs of base model and instruction finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3e05e-4976-49aa-b7b3-4459088edfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0ff70-b9e1-4c10-855f-273cfd6044ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 300)\n",
    "test_dataset[[\"messages\", \"base_assistant_message\", \"instruct_assistant_message\"]][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1497d-4e4e-49b7-9167-7f5e4633a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What an essay on Generative AI.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")#, add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "outputs = model.generate(**inputs, \n",
    "                         max_new_tokens=2000, \n",
    "                         do_sample=True, \n",
    "                         top_p=0.95, \n",
    "                         temperature=0.2, \n",
    "                         repetition_penalty=1.1, \n",
    "                         eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
